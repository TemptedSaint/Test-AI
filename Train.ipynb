{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04 22:02:00::  Caching language and country data locally...\n",
      "2018-05-04 22:02:01::  Pulling MasterData from MySQL...\n",
      "2018-05-04 22:02:05::  Training model...\n",
      "74101\n",
      "2018-05-04 22:11:04::  (74102, 16)\n",
      "Epoch 1/5000\n",
      "74102/74102 [==============================] - 5s 72us/step - loss: 0.4995 - acc: 0.5014\n",
      "Epoch 2/5000\n",
      "74102/74102 [==============================] - 5s 62us/step - loss: 0.5014 - acc: 0.4995\n",
      "Epoch 3/5000\n",
      "74102/74102 [==============================] - 5s 61us/step - loss: 0.5013 - acc: 0.4995\n",
      "Epoch 4/5000\n",
      "74102/74102 [==============================] - 5s 62us/step - loss: 0.4988 - acc: 0.5019\n",
      "Epoch 5/5000\n",
      "74102/74102 [==============================] - 5s 61us/step - loss: 0.5027 - acc: 0.4981\n",
      "Epoch 6/5000\n",
      "74102/74102 [==============================] - 5s 61us/step - loss: 0.5015 - acc: 0.4993\n",
      "Epoch 7/5000\n",
      "74102/74102 [==============================] - 4s 61us/step - loss: 0.4982 - acc: 0.5026\n",
      "Epoch 8/5000\n",
      "74102/74102 [==============================] - 4s 61us/step - loss: 0.5015 - acc: 0.4993\n",
      "Epoch 9/5000\n",
      "44000/74102 [================>.............] - ETA: 1s - loss: 0.5024 - acc: 0.4982"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a9db3490b2ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m     \u001b[0mDeepLearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-a9db3490b2ac>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampleset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplestringset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-a9db3490b2ac>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sampleset, samplestringset)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_absolute_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'RMSProp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;31m# assigning the data set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# evaluate the model still working on the version of the check that I am using, it is very inefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1712\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import functions here, remember to add to requirements.txt if a package needs to be install via pip\n",
    "import numpy as np\n",
    "import datetime\n",
    "import MySQLdb\n",
    "import sys\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Production MySql settings\n",
    "MYSQL_CREDS = {\n",
    "    \"host\": \"10.24.14.51\",\n",
    "    \"port\": 3306,\n",
    "    \"user\": \"RecruitSuggester\",\n",
    "    \"passwd\": \"HPwtdJlaLq2whEH9\",\n",
    "    \"db\": \"DeepLearning\",\n",
    "}\n",
    "\n",
    "# Uncommit the 2 lines below for remote testing.\n",
    "MYSQL_CREDS[\"host\"] = \"198.154.109.168\" # Test IP - uncomment if testing from home\n",
    "MYSQL_CREDS[\"port\"] = 33306 # Test Port - uncomment if testing from home\n",
    "\n",
    "MASTER_DATA_QUERY = \"\"\"\n",
    "SELECT\n",
    "    Handle, Moniker, ForumID, SpectrumID, Country, State,\n",
    "    Fluency0, Fluency1, Fluency2, Fluency3, Fluency4, Enlisted,\n",
    "    ForumLastActive, ChatLastActive, CustomAvatar, InviteSent, Outcome\n",
    "FROM\n",
    "    MasterData\n",
    "\"\"\"\n",
    "\n",
    "COUNTRY_QUERY = \"SELECT CountryID, CountryName from Countries\"\n",
    "LANGUAGE_QUERY = \"SELECT LanguageID, LanguageName from Languages\"\n",
    "\n",
    "\n",
    "# # Ho0ber log function, added time stamps to print\n",
    "def log(message):\n",
    "    print(\"{}::  {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), message))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "class DeepLearning(object):\n",
    "    def __init__(self):\n",
    "        self.mysql_conn = MySQLdb.connect(**MYSQL_CREDS)\n",
    "        self.languages = {}\n",
    "        self.countries = {}\n",
    "        self.pull_languages_and_countries()\n",
    "\n",
    "    def pull_languages_and_countries(self):\n",
    "        \"\"\"\n",
    "        Cache Languages and Countries tables into local dicts to save MySQL hits for\n",
    "        known values. Any new values will need to be added to these when discovered.\n",
    "        \"\"\"\n",
    "        log(\"Caching language and country data locally...\")\n",
    "        cursor = self.mysql_conn.cursor()\n",
    "\n",
    "        cursor.execute(COUNTRY_QUERY)\n",
    "        for cid,country in cursor.fetchall():\n",
    "            self.countries[country] = cid\n",
    "\n",
    "        cursor.execute(LANGUAGE_QUERY)\n",
    "        for lid,language in cursor.fetchall():\n",
    "            self.languages[language] = lid\n",
    "\n",
    "    def get_or_create_country(self, country):\n",
    "        # TODO - make this actually:\n",
    "        # - Add new countries to MySQL\n",
    "        # - Get the ID from the newly created country row\n",
    "        # - Add row to local cache dict\n",
    "        # - Return the new ID\n",
    "        return self.countries.get(country, 0) if country else None\n",
    "\n",
    "    def get_or_create_language(self, language):\n",
    "        # TODO - make this actually:\n",
    "        # - Add new languages to MySQL\n",
    "        # - Get the ID from the newly created language row\n",
    "        # - Add row to local cache dict\n",
    "        # - Return the new ID\n",
    "        return self.languages.get(language, 0) if language else None\n",
    "\n",
    "    def clean(self, row):\n",
    "        \"\"\"\n",
    "        Strip off the first two columns and numericize country and fluency fields.\n",
    "        \"\"\"\n",
    "        row = list(row) # Make the row mutable by changing to a list\n",
    "\n",
    "        # Change country into CountryID\n",
    "        row[4] = self.get_or_create_country(row[4])\n",
    "\n",
    "        # Change all fluency fields into LanguageIDs\n",
    "        for col in range(6,11):\n",
    "            row[col] = self.get_or_create_language(row[col])\n",
    "\n",
    "        # Return the resulting row, but strip off handle and moniker\n",
    "        return [col if col != '' else None for col in row[2:]]\n",
    "\n",
    "    def pull_data(self):\n",
    "        \"\"\"\n",
    "        This function pulls MasterData from MySQL and produces two numpy 2d arrays:\n",
    "        1) sampleset - A numericized version of most columns\n",
    "        2) samplestringset - Only handle and moniker as strings\n",
    "\n",
    "        This can probably be changed to just one 2d array, but this was easier to\n",
    "        interface with the train code as written.\n",
    "        \"\"\"\n",
    "        cursor = self.mysql_conn.cursor()\n",
    "        numrows = cursor.execute(MASTER_DATA_QUERY)\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Pull out just handle and moniker for samplestringset\n",
    "        names = [[r[0], r[1]] for r in results]\n",
    "\n",
    "        # Strip off the first two columns and numericize country and fluency fields\n",
    "        numbers = [self.clean(row) for row in results]\n",
    "\n",
    "        # I went for np.array directly rather than fromiter because fromiter was finicky\n",
    "        # We might need to change that, but I'm uncertain\n",
    "        sampleset = np.array(numbers)\n",
    "        samplestringset = np.array(names)\n",
    "\n",
    "        return sampleset, samplestringset\n",
    "\n",
    "    def train(self, sampleset, samplestringset):\n",
    "        \"\"\"\n",
    "        This is TemptedSaint's code, nearly verbatim. I can't comment on it\n",
    "        other than to note that I changed a bunch of 16s to 15s after we cut\n",
    "        out our internal identifier. I wasn't certain if any other numbers needed\n",
    "        to be shifted as a result.\n",
    "\n",
    "        I know I borked something, as the accuracy is 0% when I run this.\n",
    "        \"\"\"\n",
    "        sampleset[sampleset == None] = 0\n",
    "        trainset = np.empty((0))\n",
    "\n",
    "        #removing of the 5 and converting 2 to 1\n",
    "        #changing 1 to 0, 3 to 1 and 4 to 2\n",
    "        #changing names to comparison values\n",
    "\n",
    "        rowcount,colcount = sampleset.shape\n",
    "        rawcount,x = sampleset.shape\n",
    "        rowcount -=1\n",
    "        colcount -=1\n",
    "        print (rowcount)\n",
    "        while (rowcount > -1):\n",
    "            handle = samplestringset[rowcount,0]\n",
    "            moniker = samplestringset[rowcount,1]\n",
    "            hanlen = len(handle)\n",
    "            monlen = len(moniker)\n",
    "            tempset = np.empty((0))\n",
    "            # charecter count\n",
    "            charcnt = int(0)\n",
    "            # charecter comparison count\n",
    "            compcnt = int(0)\n",
    "            # total comparison float\n",
    "            strcomp = float(0)\n",
    "            if hanlen >= monlen:\n",
    "                while (charcnt < monlen):\n",
    "                    if (handle[charcnt] == moniker[charcnt]):\n",
    "                        compcnt += 1\n",
    "                    charcnt += 1\n",
    "                strcomp = compcnt / charcnt\n",
    "            else:\n",
    "                while (charcnt < hanlen):\n",
    "                    if handle[charcnt] == moniker[charcnt]:\n",
    "                        compcnt += 1\n",
    "                    charcnt += 1\n",
    "                strcomp = compcnt/charcnt\n",
    "            a = np.empty((0))\n",
    "            a = np.append(a, strcomp)\n",
    "            #this will create the entire new array with the strings accounted for in comparison with recruiting value at the end\n",
    "            tempset = np.empty((0))\n",
    "            tempset = np.append(tempset, values=sampleset[rowcount, :colcount])\n",
    "            tempset = np.append(tempset, values=[strcomp])\n",
    "            tempset = np.append(tempset, values=sampleset[rowcount, colcount])\n",
    "            if ((rowcount+1) == rawcount):\n",
    "                trainset = tempset\n",
    "            else:\n",
    "                trainset = np.vstack([trainset, tempset])\n",
    "            rowcount -= 1\n",
    "\n",
    "        log(trainset.shape)\n",
    "        # converting the data set so that it can be used for training\n",
    "        colcount += 1\n",
    "        traindata,checkdata = trainset[:,:colcount],trainset[:,colcount]\n",
    "        # build nueral net\n",
    "        #rowcount = rawcount\n",
    "        #checktb = np.zeros((rowcount,2), dtype=float)\n",
    "        #print (colcount)\n",
    "        #print (traindata.shape)\n",
    "        #rowcount-=1\n",
    "        #while (rowcount>-1):\n",
    "            #if(checkdata[rowcount]==1):\n",
    "                #checktb[rowcount,1]=1\n",
    "            #else:\n",
    "                #checktb[rowcount,0]=1\n",
    "            #rowcount-=1\n",
    "        model = Sequential()\n",
    "        model.add(Dense(colcount, input_dim=colcount,  activation='relu'))\n",
    "        model.add(Dense(1000, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(500, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(250, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(125,activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(25,activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # variable loss with optimizer\n",
    "        model.compile(loss='mean_absolute_error', optimizer='RMSProp', metrics=['accuracy'])\n",
    "        # assigning the data set\n",
    "        model.fit(traindata, checkdata, epochs=5000, batch_size=1000)\n",
    "\n",
    "        # evaluate the model still working on the version of the check that I am using, it is very inefficient\n",
    "        scores = model.evaluate(traindata, checkdata)\n",
    "        log(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        \n",
    "        #rowcount,colcnt = trainset.shape\n",
    "        #resultstb = np.zeros((rowcount,2), dtype=float)\n",
    "        #rowcount-=1\n",
    "        model.save('adi.hd5')\n",
    "        results = np.zeros((0))\n",
    "        prdctdata= model.predict(traindata)\n",
    "        np.concatenate((prdctdata,checkdata), axis=0, out=results)\n",
    "        np.savetxt('results.txt',results, delimiter=' ', newline='\\n', header='', footer='', comments='# ', encoding=None)\n",
    "        #print(prdctdata.shape)\n",
    "        #ocount=0\n",
    "        #correct1=0\n",
    "        #zcount=0\n",
    "        #correct0=0\n",
    "        #loop to get results\n",
    "        \n",
    "        #while (rowcount > -1):\n",
    "            #if (checktb[rowcount,0]==1):\n",
    "                #if (prdctdata[rowcount,0]>prdctdata[rowcount,1]):\n",
    "                    #zcount +=1\n",
    "                    #correct0 +=1\n",
    "                #else:\n",
    "                    #zcount +=1\n",
    "                    \n",
    "            #else:\n",
    "                #log(prdctdata[rowcount,1])\n",
    "                #log(prdctdata[rowcount,0])\n",
    "                #log(\"next\")\n",
    "                #if (prdctdata[rowcount,1]>prdctdata[rowcount,0]):\n",
    "                    #ocount+=1\n",
    "                    #correct1 +=1\n",
    "                #else:\n",
    "                    #ocount +=1\n",
    "            #rowcount-=1\n",
    "        #print (correct0)\n",
    "        #print (zcount)\n",
    "        #print (correct1)\n",
    "        #print (ocount)\n",
    "        \n",
    "        #saves the model\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Main entry-point for the class. pull_data into numpy arrays, then pass them off to train\n",
    "        \"\"\"\n",
    "        log(\"Pulling MasterData from MySQL...\")\n",
    "        sampleset, samplestringset = self.pull_data()\n",
    "\n",
    "        log(\"Training model...\")\n",
    "        self.train(sampleset, samplestringset)\n",
    "\n",
    "        log(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DeepLearning().run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
