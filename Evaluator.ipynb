{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-04 22:41:21::  Caching language and country data locally...\n",
      "2018-05-04 22:41:21::  Pulling MasterData from MySQL...\n",
      "2018-05-04 22:41:25::  Training model...\n",
      "2018-05-04 22:49:17::  (74102, 16)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f2a843fbcc92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[0mDeepLearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-f2a843fbcc92>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampleset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplestringset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-f2a843fbcc92>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sampleset, samplestringset)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mrowcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrowcount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[0mprdctdata\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprdctdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheckdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Import functions here, remember to add to requirements.txt if a package needs to be install via pip\n",
    "import numpy as np\n",
    "import datetime\n",
    "import MySQLdb\n",
    "import sys\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# Production MySql settings\n",
    "MYSQL_CREDS = {\n",
    "    \"host\": \"10.24.14.51\",\n",
    "    \"port\": 3306,\n",
    "    \"user\": \"RecruitSuggester\",\n",
    "    \"passwd\": \"HPwtdJlaLq2whEH9\",\n",
    "    \"db\": \"DeepLearning\",\n",
    "}\n",
    "\n",
    "# Uncommit the 2 lines below for remote testing.\n",
    "MYSQL_CREDS[\"host\"] = \"198.154.109.168\" # Test IP - uncomment if testing from home\n",
    "MYSQL_CREDS[\"port\"] = 33306 # Test Port - uncomment if testing from home\n",
    "\n",
    "MASTER_DATA_QUERY = \"\"\"\n",
    "SELECT\n",
    "    Handle, Moniker, ForumID, SpectrumID, Country, State,\n",
    "    Fluency0, Fluency1, Fluency2, Fluency3, Fluency4, Enlisted,\n",
    "    ForumLastActive, ChatLastActive, CustomAvatar, InviteSent, Outcome\n",
    "FROM\n",
    "    MasterData\n",
    "\"\"\"\n",
    "\n",
    "COUNTRY_QUERY = \"SELECT CountryID, CountryName from Countries\"\n",
    "LANGUAGE_QUERY = \"SELECT LanguageID, LanguageName from Languages\"\n",
    "\n",
    "\n",
    "# # Ho0ber log function, added time stamps to print\n",
    "def log(message):\n",
    "    print(\"{}::  {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), message))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "class DeepLearning(object):\n",
    "    def __init__(self):\n",
    "        self.mysql_conn = MySQLdb.connect(**MYSQL_CREDS)\n",
    "        self.languages = {}\n",
    "        self.countries = {}\n",
    "        self.pull_languages_and_countries()\n",
    "\n",
    "    def pull_languages_and_countries(self):\n",
    "        \"\"\"\n",
    "        Cache Languages and Countries tables into local dicts to save MySQL hits for\n",
    "        known values. Any new values will need to be added to these when discovered.\n",
    "        \"\"\"\n",
    "        log(\"Caching language and country data locally...\")\n",
    "        cursor = self.mysql_conn.cursor()\n",
    "\n",
    "        cursor.execute(COUNTRY_QUERY)\n",
    "        for cid,country in cursor.fetchall():\n",
    "            self.countries[country] = cid\n",
    "\n",
    "        cursor.execute(LANGUAGE_QUERY)\n",
    "        for lid,language in cursor.fetchall():\n",
    "            self.languages[language] = lid\n",
    "\n",
    "    def get_or_create_country(self, country):\n",
    "        # TODO - make this actually:\n",
    "        # - Add new countries to MySQL\n",
    "        # - Get the ID from the newly created country row\n",
    "        # - Add row to local cache dict\n",
    "        # - Return the new ID\n",
    "        return self.countries.get(country, 0) if country else None\n",
    "\n",
    "    def get_or_create_language(self, language):\n",
    "        # TODO - make this actually:\n",
    "        # - Add new languages to MySQL\n",
    "        # - Get the ID from the newly created language row\n",
    "        # - Add row to local cache dict\n",
    "        # - Return the new ID\n",
    "        return self.languages.get(language, 0) if language else None\n",
    "\n",
    "    def clean(self, row):\n",
    "        \"\"\"\n",
    "        Strip off the first two columns and numericize country and fluency fields.\n",
    "        \"\"\"\n",
    "        row = list(row) # Make the row mutable by changing to a list\n",
    "\n",
    "        # Change country into CountryID\n",
    "        row[4] = self.get_or_create_country(row[4])\n",
    "\n",
    "        # Change all fluency fields into LanguageIDs\n",
    "        for col in range(6,11):\n",
    "            row[col] = self.get_or_create_language(row[col])\n",
    "\n",
    "        # Return the resulting row, but strip off handle and moniker\n",
    "        return [col if col != '' else None for col in row[2:]]\n",
    "\n",
    "    def pull_data(self):\n",
    "        \"\"\"\n",
    "        This function pulls MasterData from MySQL and produces two numpy 2d arrays:\n",
    "        1) sampleset - A numericized version of most columns\n",
    "        2) samplestringset - Only handle and moniker as strings\n",
    "\n",
    "        This can probably be changed to just one 2d array, but this was easier to\n",
    "        interface with the train code as written.\n",
    "        \"\"\"\n",
    "        cursor = self.mysql_conn.cursor()\n",
    "        numrows = cursor.execute(MASTER_DATA_QUERY)\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Pull out just handle and moniker for samplestringset\n",
    "        names = [[r[0], r[1]] for r in results]\n",
    "\n",
    "        # Strip off the first two columns and numericize country and fluency fields\n",
    "        numbers = [self.clean(row) for row in results]\n",
    "\n",
    "        # I went for np.array directly rather than fromiter because fromiter was finicky\n",
    "        # We might need to change that, but I'm uncertain\n",
    "        sampleset = np.array(numbers)\n",
    "        samplestringset = np.array(names)\n",
    "\n",
    "        return sampleset, samplestringset\n",
    "\n",
    "    def train(self, sampleset, samplestringset):\n",
    "        \"\"\"\n",
    "        This is TemptedSaint's code, nearly verbatim. I can't comment on it\n",
    "        other than to note that I changed a bunch of 16s to 15s after we cut\n",
    "        out our internal identifier. I wasn't certain if any other numbers needed\n",
    "        to be shifted as a result.\n",
    "\n",
    "        I know I borked something, as the accuracy is 0% when I run this.\n",
    "        \"\"\"\n",
    "\n",
    "        sampleset[sampleset == None] = 0\n",
    "        trainset = np.empty((0))\n",
    "\n",
    "        #removing of the 5 and converting 2 to 1\n",
    "        #changing 1 to 0, 3 to 1 and 4 to 2\n",
    "        #changing names to comparison values\n",
    "\n",
    "        rowcount,colcount = sampleset.shape\n",
    "        rawcount,x = sampleset.shape\n",
    "        rowcount -=1\n",
    "        colcount -=1\n",
    "        while (rowcount > -1):\n",
    "            handle = samplestringset[rowcount,0]\n",
    "            moniker = samplestringset[rowcount,1]\n",
    "            hanlen = len(handle)\n",
    "            monlen = len(moniker)\n",
    "            tempset = np.empty((0))\n",
    "            # charecter count\n",
    "            charcnt = int(0)\n",
    "            # charecter comparison count\n",
    "            compcnt = int(0)\n",
    "            # total comparison float\n",
    "            strcomp = float(0)\n",
    "            if hanlen >= monlen:\n",
    "                while (charcnt < monlen):\n",
    "                    if (handle[charcnt] == moniker[charcnt]):\n",
    "                        compcnt += 1\n",
    "                    charcnt += 1\n",
    "                strcomp = compcnt / charcnt\n",
    "            else:\n",
    "                while (charcnt < hanlen):\n",
    "                    if handle[charcnt] == moniker[charcnt]:\n",
    "                        compcnt += 1\n",
    "                    charcnt += 1\n",
    "                strcomp = compcnt/charcnt\n",
    "            a = np.empty((0))\n",
    "            a = np.append(a, strcomp)\n",
    "            #this will create the entire new array with the strings accounted for in comparison with recruiting value at the end\n",
    "            tempset = np.empty((0))\n",
    "            tempset = np.append(tempset, values=sampleset[rowcount, :colcount])\n",
    "            tempset = np.append(tempset, values=[strcomp])\n",
    "            tempset = np.append(tempset, values=sampleset[rowcount, colcount])\n",
    "            if ((rowcount+1) == rawcount):\n",
    "                trainset = tempset\n",
    "            else:\n",
    "                trainset = np.vstack([trainset, tempset])\n",
    "            rowcount -= 1\n",
    "\n",
    "        log(trainset.shape)\n",
    "\n",
    "        # converting the data set so that it can be used for training\n",
    "        colcount += 1\n",
    "        traindata,checkdata = trainset[:,:colcount],trainset[:,colcount]\n",
    "\n",
    "        # load neural net\n",
    "        model =load_model('adi.hd5')\n",
    "        \n",
    "        rowcount=checkdata.shape\n",
    "        results = np.zeros((rowcount,2))\n",
    "        prdctdata= model.predict(traindata)\n",
    "        np.concatenate((prdctdata,checkdata), axis=0, out=results)\n",
    "        np.savetxt('results.txt',results, delimiter=' ', newline='\\n', header='', footer='', comments='# ', encoding=None)\n",
    "        print (ocount)\n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Main entry-point for the class. pull_data into numpy arrays, then pass them off to train\n",
    "        \"\"\"\n",
    "        log(\"Pulling MasterData from MySQL...\")\n",
    "        sampleset, samplestringset = self.pull_data()\n",
    "\n",
    "        log(\"Training model...\")\n",
    "        self.train(sampleset, samplestringset)\n",
    "\n",
    "        log(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DeepLearning().run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
